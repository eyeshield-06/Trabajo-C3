\documentclass[b4paper]{article}
\usepackage[margin=2cm]{geometry}
\usepackage[spanish]{babel}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{fancybox}
\usepackage{color}
\usepackage{tcolorbox}
\setlength{\parindent}{0cm}
\begin{document}
    \begin{figure}
        \includegraphics[scale=0.25]{/home/eyeshield/Downloads/Logo.png}
    \end{figure}
    \begin{center}
        \shadowbox{\huge \textbf{Trabajo}}\\[0.2cm]
        \shadowbox{\huge \textbf{Analisis Exploratorio de Datos}}\\[1cm]
        \begin{minipage}{0.4\textwidth}
            \begin{flushleft} \large
                \emph{\textbf{Docente:}}\\
                \textup{Luis Guzman}
            \end{flushleft}
        \end{minipage}
        ~
        \begin{minipage}{0.4\textwidth}
            \begin{flushright} \large
                \emph{\textbf{Estudiantes:}} \\
                \textup{Felipe \'Avila}\\
                \textup{Rudy Miranda}
            \end{flushright}
        \end{minipage}\\[1cm]
        \makeatother
    \end{center}
    \tableofcontents
    \listoffigures
    \listoftables

    \newpage
    \section{Carga de Datos}
    \begin{tcolorbox}
        \begin{verbatim}
getwd()
setwd("/home/eyeshield/Documents/A.E.D/trabajo")
datos  <-  read.csv("bee_colony.csv")\end{verbatim}
    \end{tcolorbox}
    \section{Ejercicio 1}
    \subsection{a)}
    \begin{tcolorbox}
        \begin{verbatim}
library(funModeling)
df_status(datos)\end{verbatim}
    \end{tcolorbox}
    \begin{table}[h]
        \centering
\begin{tabular}{l|l|l|l|l|l|l|l|l|l|}
\cline{2-10}
                                  & \textbf{variable} & \textbf{q\_zeros} & \textbf{p\_zeros} & \textbf{q\_na} & \textbf{p\_na} & \textbf{q\_inf} & \textbf{p\_inf} & \textbf{type} & \textbf{unique} \\ \hline
\multicolumn{1}{|l|}{\textbf{1}}  & year              & 0                 & 0                 & 0              & 0.00           & 0               & 0               & integer       & 7               \\ \hline
\multicolumn{1}{|l|}{\textbf{2}}  & months            & 0                 & 0                 & 0              & 0.00           & 0               & 0               & character     & 4               \\ \hline
\multicolumn{1}{|l|}{\textbf{3}}  & state             & 0                 & 0                 & 0              & 0.00           & 0               & 0               & character     & 47              \\ \hline
\multicolumn{1}{|l|}{\textbf{4}}  & colony\_n         & 0                 & 0                 & 47             & 3.85           & 0               & 0               & integer       & 284             \\ \hline
\multicolumn{1}{|l|}{\textbf{5}}  & colony\_max       & 0                 & 0                 & 72             & 5.89           & 0               & 0               & integer       & 278             \\ \hline
\multicolumn{1}{|l|}{\textbf{6}}  & colony\_lost      & 0                 & 0                 & 47             & 3.85           & 0               & 0               & integer       & 254             \\ \hline
\multicolumn{1}{|l|}{\textbf{7}}  & colony\_lost\_pct & 0                 & 0                 & 54             & 4.42           & 0               & 0               & integer       & 42              \\ \hline
\multicolumn{1}{|l|}{\textbf{8}}  & colony\_added     & 0                 & 0                 & 83             & 6.79           & 0               & 0               & integer       & 256             \\ \hline
\multicolumn{1}{|l|}{\textbf{9}}  & colony\_reno      & 0                 & 0                 & 131            & 10.72          & 0               & 0               & integer       & 252             \\ \hline
\multicolumn{1}{|l|}{\textbf{10}} & colony\_reno\_pct & 0                 & 0                 & 260            & 21.28          & 0               & 0               & integer       & 54              \\ \hline
\end{tabular}
        \caption{An\'alisis exploratorio}
\end{table}
    \begin{tcolorbox}
        \begin{verbatim}
nna_datos  <- na.omit(datos)
df_status(nna_datos)\end{verbatim}
    \end{tcolorbox}
    \begin{table}[h]
        \centering
\begin{tabular}{l|l|l|l|l|l|l|l|l|l|}
\cline{2-10}
                                  & \textbf{variable} & \textbf{q\_zeros} & \textbf{p\_zeros} & \textbf{q\_na} & \textbf{p\_na} & \textbf{q\_inf} & \textbf{p\_inf} & \textbf{type} & \textbf{unique} \\ \hline
\multicolumn{1}{|l|}{\textbf{1}}  & year              & 0                 & 0                 & 0              & 0              & 0               & 0               & integer       & 7               \\ \hline
\multicolumn{1}{|l|}{\textbf{2}}  & months            & 0                 & 0                 & 0              & 0              & 0               & 0               & character     & 4               \\ \hline
\multicolumn{1}{|l|}{\textbf{3}}  & state             & 0                 & 0                 & 0              & 0              & 0               & 0               & character     & 46              \\ \hline
\multicolumn{1}{|l|}{\textbf{4}}  & colony\_n         & 0                 & 0                 & 0              & 0              & 0               & 0               & integer       & 241             \\ \hline
\multicolumn{1}{|l|}{\textbf{5}}  & colony\_max       & 0                 & 0                 & 0              & 0              & 0               & 0               & integer       & 258             \\ \hline
\multicolumn{1}{|l|}{\textbf{6}}  & colony\_lost      & 0                 & 0                 & 0              & 0              & 0               & 0               & integer       & 223             \\ \hline
\multicolumn{1}{|l|}{\textbf{7}}  & colony\_lost\_pct & 0                 & 0                 & 0              & 0              & 0               & 0               & integer       & 39              \\ \hline
\multicolumn{1}{|l|}{\textbf{8}}  & colony\_added     & 0                 & 0                 & 0              & 0              & 0               & 0               & integer       & 228             \\ \hline
\multicolumn{1}{|l|}{\textbf{9}}  & colony\_reno      & 0                 & 0                 & 0              & 0              & 0               & 0               & integer       & 226             \\ \hline
\multicolumn{1}{|l|}{\textbf{10}} & colony\_reno\_pct & 0                 & 0                 & 0              & 0              & 0               & 0               & integer       & 54              \\ \hline
\end{tabular}
        \caption{data frame limpio}
\end{table}
    \begin{tcolorbox}
        \begin{verbatim}
library(pastecs)
round(stat.desc(nna_datos), digits = 2)\end{verbatim}
    \end{tcolorbox}
    
    \begin{table}[h]
        \tiny
        \centering
\begin{tabular}{l|l|l|l|l|l|l|l|l|l|l|}
\cline{2-11}
                                        & \textbf{year} & \textbf{months} & \textbf{state} & \textbf{colony\_n} & \textbf{colony\_max} & \textbf{colony\_lost} & \textbf{colony\_lost\_pct} & \textbf{colony\_added} & \textbf{colony\_reno} & \textbf{colony\_reno\_pct} \\ \hline
\multicolumn{1}{|l|}{\textbf{nbr.val}}  & 929           & NA              & NA             & 929                & 929                  & 929                   & 929                        & 929                    & 929                   & 929                        \\ \hline
\multicolumn{1}{|l|}{\textbf{nbr.null}} & 0             & NA              & NA             & 0                  & 0                    & 0                     & 0                          & 0                      & 0                     & 0                          \\ \hline
\multicolumn{1}{|l|}{\textbf{nbr.na}}   & 0             & NA              & NA             & 0                  & 0                    & 0                     & 0                          & 0                      & 0                     & 0                          \\ \hline
\multicolumn{1}{|l|}{\textbf{min}}      & 2015          & NA              & NA             & 1.6e+06            & 190                  & 30                    & 1                          & 10                     & 20                    & 1                          \\ \hline
\multicolumn{1}{|l|}{\textbf{max}}      & 2021          & NA              & NA             & 1.44e+06           & 1.71+e06             & 255000                & 48                         & 250000                 & 2.85+e05              & 77                         \\ \hline
\multicolumn{1}{|l|}{\textbf{range}}    & 6             & NA              & NA             & 1.43+e06           & 1.7+e06              & 254970                & 47                         & 249990                 & 2.84e+05              & 76                         \\ \hline
\multicolumn{1}{|l|}{\textbf{median}}   & 2018          & NA              & NA             & 1.75+e04           & 2.2+e04              & 2200                  & 10                         & 2100                   & 1.3e+03               & 6                          \\ \hline
\multicolumn{1}{|l|}{\textbf{mean}}     & 2017.77       & NA              & NA             & 1.07+e04           & 1.34+e04             & 9646.37               & 11.46                      & 10329.04               & 8.95e+03              & 9.08                       \\ \hline
\multicolumn{1}{|l|}{\textbf{var}}      & 3.36          & NA              & NA             & 2.79+e10           & 4.37+e10             & 661651420.77          & 49.79                      & 736043769.45           & 6.44e+08              & 94.64                      \\ \hline
\multicolumn{1}{|l|}{\textbf{std.dev}}  & 1.91          & NA              & NA             & 1.67+e05           & 2.09+e05             & 25722.59              & 7.06                       & 27130.13               & 2.53e+04              & 9.73                       \\ \hline
\multicolumn{1}{|l|}{\textbf{coef.var}} & 0             & NA              & NA             & 2.37               & 2.36                 & 2.67                  & 0.62                       & 2.63                   & 2.84                  & 1.07                       \\ \hline
\end{tabular}
        \caption{An\'alisis descriptivo}
\end{table}
    \normalsize
    Por otro lado, los datos ser\'an escalados al momento de realizar clusters.
    \subsection{b)}
    \begin{tcolorbox}[title = input]
        \begin{verbatim}
attach(nna_datos) #columnas como objetos del entorno
model  <- lm(colony_n ~ colony_lost) #Regresion lineal simple
model_log  <- lm(colony_n ~ log(colony_lost)) #model log lineal
model_loess  <- loess(colony_n ~ colony_lost) #regresion ponderada localmente\end{verbatim}
    \end{tcolorbox}
    \subsection{c)}
    Al ejecutar la funci\'on \textit{girafe} de la librer\'ia \textit{ggiraph} en linux
    obtenemos un error. La soluci\'on de este la encontramos en \url{https://rdrr.io/r/utils/browseURL.html}, donde solo tenemos que ejecutar en nuestra terminal con \textit{R} corriendo \verb|options(browser = "firefox")|.
    \begin{tcolorbox}[title = input]
        \begin{verbatim}
library(ggplot2)
library(ggiraph)
gg_lm  <- ggplot(nna_datos, aes(x = colony_lost, y = colony_n)) +
    ggtitle("Modelo Lineal") +
    geom_point_interactive() +
    stat_smooth(method = "lm", col = "blue")

gg_log  <- ggplot(nna_datos, aes(x = log(colony_lost), y = colony_n)) +
    ggtitle("Modelo Loglineal") +
    geom_point_interactive() +
    stat_smooth(method = "lm", col = "blue")

gg_loess  <- ggplot(nna_datos, aes(x = colony_lost, y = colony_n)) +
    ggtitle("Modelo Loess") +
    geom_point_interactive() +
    stat_smooth(method = "loess", col = "blue")

girafe(ggobj = gg_lm)
girafe(ggobj = gg_log)
girafe(ggobj = gg_loess)\end{verbatim}
    \end{tcolorbox}
    \begin{figure}[h]
        \begin{minipage}[c]{0.33\textwidth}
            \centering
                \includegraphics[width=0.8\textwidth]{uno.png}
                \caption{M. Lineal}
        \end{minipage}
        \begin{minipage}[c]{0.33\textwidth}
            \centering
                \includegraphics[width=0.8\textwidth]{dos.png}
                \caption{M. LogLineal}
        \end{minipage}
        \begin{minipage}[c]{0.33\textwidth}
            \centering
                \includegraphics[width=0.8\textwidth]{tres.png}
                \caption{M. Loess}
        \end{minipage}
    \end{figure}
    \subsection{d)}
    \begin{tcolorbox}[title = input]
        \begin{verbatim}
summary(model)$sigma
summary(model_log)$sigma
summary(model_loess)$s\end{verbatim}
    \end{tcolorbox}
    \begin{tcolorbox}[title = output]
        \begin{verbatim}
> summary(model)$sigma
[1] 66068.51
> summary(model_log)$sigma
[1] 127610.2
> summary(model_loess)$s
[1] 60349.14\end{verbatim}
    \end{tcolorbox}

    Fij\'andonos en la suma de los residuos, entonces el modelo que mejor se ajusta a los
    datos es el de regresi\'on ponderada localmente (\verb|model_loess|).
    \subsection{e)}
    Para generar los n\'umeros aleatorios nos apoyaremos de la distribuci\'on uniforme, donde
    los par\'ametros ser\'an el valor m\'inimo y m\'aximo de las variables \textit{colony\_lost}.
    \begin{tcolorbox}[title = input]
        \begin{verbatim}
#Numeros aleatorio con runif
new_data <- sort(runif(10, min = min(colony_lost), max = max(colony_lost)))
predict(model_loess, new_data) #Predicciones\end{verbatim}
    \end{tcolorbox}
    \begin{figure}[h]
    \begin{minipage}[c]{0.5\textwidth}
    \begin{tcolorbox}[title = output]
        \begin{verbatim}
> cbind(new_data, predict(model_loess, new_data))
       new_data          
 [1,]  69678.62  480021.7
 [2,]  87559.58  582172.7
 [3,] 106706.83  682813.9
 [4,] 124818.13  770726.8
 [5,] 182070.19 1013448.4
 [6,] 183339.79 1018384.0
 [7,] 201319.32 1086938.7
 [8,] 205476.97 1102494.7
 [9,] 234381.95 1208925.2
[10,] 242686.62 1239304.7\end{verbatim}
    \end{tcolorbox}
    \end{minipage}
    \begin{minipage}[r]{0.4\textwidth}
        \centering
        \includegraphics[width = 0.8\textwidth]{mi_plot.png}
        \caption{Predicciones}
    \end{minipage}
    \end{figure}
    \subsection{f)}
    \begin{minipage}[c]{0.5\textwidth}
        \begin{tcolorbox}[title = input]
            \begin{verbatim}
severalv_model <- lm(colony_n ~ year + colony_max +
colony_lost + colony_lost_pct + colony_added +
colony_reno + colony_reno_pct)
summary(severalv_model)$sigma\end{verbatim}
        \end{tcolorbox}
    \end{minipage}
    \begin{minipage}[c][2.94cm][t]{0.4\textwidth}
        \begin{tcolorbox}[title = output]
            \begin{verbatim}
> summary(severalv_model)$sigma
[1] 34798.53\end{verbatim}
        \end{tcolorbox}
    \end{minipage}

    Notar que solo se hizo uso de las variables cuantitativas, pero de todos modos se
    obtuvo un modelo mucho mejor que los anteriores, con un error casi de la mitad que
    el obtenido por el m\'etodo \textit{loess}.
    \section{Ejercicio 3}
    Haremos primero el tres, ya que se podr\'ia dar el caso de que nos entregue que la cantidad optima de clusters sea de dos, lo cual nos puede dar una idea de como realizar el ejercicio 2, ya que de momento no se tiene una noci\'on clara de como abordarlo, puesto que las dos variables cualitativas que tenemos, ninguna de ellas es dicot\'omica.
    
    Volvemos a cargar nuestros datos.
    \begin{tcolorbox}
        \begin{verbatim}
datos  <-  read.csv("bee_colony.csv") #cargamos .csv
nna_datos <- na.omit(datos) # se quitar na
trimestres <- nna_datos[, 2] # guardamos variables trimestres
nna_datos <- nna_datos[,-2] # se quita de nuestro conjunto
nna_datos$state <- as.factor(nna_datos$state) # estados a factor
datos <- scale(nna_datos[,-2]) # se escalan todas las variables,
# menos estado, que es factor
datos <- cbind(datos,nna_datos$state) # se unen nuevamente\end{verbatim}
    \end{tcolorbox}
    Partiremos con el m\'etodo jer\'arquico acumulativo.
    \subsection{Metodos Jerarquicos}
    \subsubsection{Metodo Jerarquico Acumulativo}
    \begin{tcolorbox}
        \begin{verbatim}
# Matriz de distancias euclídeas
mat_dist <- dist(x = datos, method = "euclidean")
# Dendrogramas con linkage complete y average
hc_complete <- hclust(d = mat_dist, method = "complete")
hc_average <- hclust(d = mat_dist, method = "average")\end{verbatim}
    \end{tcolorbox}
    Ahora evaluamos hasta que punto nuestros dendrogramas reflejan las verdaderas
    distancias entre las observaciones con la funci\'on \verb|cor|
    \begin{minipage}{0.5\textwidth}
        \begin{tcolorbox}[title = input]
            \begin{verbatim}
cor(x = mat_dist, cophenetic(hc_complete))
cor(x = mat_dist, cophenetic(hc_average))\end{verbatim}
        \end{tcolorbox}
    \end{minipage}
    \begin{minipage}{0.5\textwidth}
        \begin{tcolorbox}[title = output]
            \begin{verbatim}
> cor(x = mat_dist, cophenetic(hc_complete))
[1] 0.7236253
> cor(x = mat_dist, cophenetic(hc_average))
[1] 0.7184969\end{verbatim}
        \end{tcolorbox}
    \end{minipage}
    Los cuales no son valores del todo buenos. De todas formas, procedemos a graficar.
    \begin{tcolorbox}[title = input]
        \begin{verbatim}
library(magrittr)
library(factoextra)
#2 clusters
fviz_dend(x = hc_complete, k = 2, cex = 0.6)
#4 clusters
fviz_dend(x = hc_complete, k = 4, cex = 0.6)\end{verbatim}
    \end{tcolorbox}
    \begin{figure}[h]
        \begin{minipage}[c]{0.5\textwidth}
            \centering
                \includegraphics[width=0.8\textwidth]{dendo1.png}
                \caption{2 Clusters}
        \end{minipage}
        \begin{minipage}[c]{0.5\textwidth}
            \centering
                \includegraphics[width=0.8\textwidth]{dendo2.png}
                \caption{4 Clusters}
        \end{minipage}
    \end{figure}
    Nuestra esperanza era que se distribuyeran en los distintos clusters los trimestres.
    Ahora veremos si los resultados cumplieron con ello.
    \begin{tcolorbox}[title = input]
        \begin{verbatim}
clusters = cutree(tree = hc_complete, k = 2)
table(clusters, trimestres, dnn = list("clusters", "trimestres"))
clusters = cutree(tree = hc_complete, k = 4)
table(clusters, trimestres, dnn = list("clusters", "trimestres"))\end{verbatim}
    \end{tcolorbox}
    \begin{tcolorbox}[title = output]
        \begin{verbatim}
> clusters = cutree(tree = hc_complete, k = 2)
> table(clusters, trimestres, dnn = list("clusters", "trimestres"))
        trimestres
clusters April-June January-March July-September October-December
       1         88            82             92               65
       2        185           134            173              110
> clusters = cutree(tree = hc_complete, k = 4)
> table(clusters, trimestres, dnn = list("clusters", "trimestres"))
        trimestres
clusters April-June January-March July-September October-December
       1         82            75             86               59
       2          6             7              6                6
       3         84            57             75               47
       4        101            77             98               63\end{verbatim}
    \end{tcolorbox}
    Tanto para dos, como para cuatro clusters, podemos notar que los trimestres no
    se asignan a un cluster en particular, por lo que el algoritmo debe de estar 
    apuntando a otra cosa.
    \subsection{Metodos no Jerarquicos}
    Comenzaremos usando la funci\'on \verb|clValid| de la librer\'ia con el mismo nombre, para
    as\'i tener una de idea de la cantidad de clusters y en que m\'etodo aplicarlos.
    \begin{tcolorbox}
        \begin{verbatim}
library(clValid)
comparacion <- clValid(obj = datos, nClust = c(2, 3, 4),
    clMethods = c("hierarchical", "kmeans", "pam"), validation = c("stability", "internal"))
summary(comparacion)\end{verbatim}
    \end{tcolorbox}
    \newpage
    \begin{table}[h]
        \centering
\begin{tabular}{l|l|l|l|}
\cline{2-4}
                                            & \textbf{Score} & \textbf{Method} & \textbf{Clusters} \\ \hline
\multicolumn{1}{|l|}{\textbf{APN}}          & 0.0249         & kmeans          & 2                 \\ \hline
\multicolumn{1}{|l|}{\textbf{AD}}           & 6.7405         & pam             & 4                 \\ \hline
\multicolumn{1}{|l|}{\textbf{ADM}}          & 1.2041         & pam             & 3                 \\ \hline
\multicolumn{1}{|l|}{\textbf{FOM}}          & 2.0976         & kmeans          & 4                 \\ \hline
\multicolumn{1}{|l|}{\textbf{Connectivity}} & 10.7472        & pam             & 2                 \\ \hline
\multicolumn{1}{|l|}{\textbf{Dunn}}         & 0.0633         & kmeans          & 4                 \\ \hline
\multicolumn{1}{|l|}{\textbf{Silhouette}}   & 0.5978         & pam             & 2                 \\ \hline
\end{tabular}
        \caption{Optimal Scores}
\end{table}
    \subsubsection{K-means}
    De la tabla anterior, tenemos que dentro de nuestro rango de cluster, la mejor combinaci\'on que podr\'iamos hacer es \textit{Kmeans} con 2 clusters. Esto lo corroboraremos con una representacion visual.
    \begin{tcolorbox}
        \begin{verbatim}
fviz_nbclust(x = datos, FUNcluster = kmeans, method = "silhouette") +
        labs(title = "Número óptimo de clusters")\end{verbatim}
    \end{tcolorbox}
    \begin{figure}[h]
        \centering
        \includegraphics[width = 0.7\textwidth]{howmanyK.png}
        \caption{Numeros Optimo de Clusters}
    \end{figure}
    \begin{tcolorbox}
        \begin{verbatim}
km_clusters <- kmeans(x = datos, centers = 2, nstart = 25)
fviz_cluster(object = km_clusters, data = datos, show.clust.cent = TRUE,
    ellipse.type = "euclid", star.plot = TRUE, repel = TRUE) + theme_bw()\end{verbatim}
    \end{tcolorbox}
    \newpage
    \begin{figure}[h]
        \centering
        \includegraphics[width = 0.7\textwidth]{kmm.png}
        \caption{Kmeans k = 2}
    \end{figure}
    \subsubsection{PAM}
    \begin{tcolorbox}
        \begin{verbatim}
library(cluster)
pam_clusters = pam(x = datos, k = 4, metric = "euclidean")
fviz_cluster(object = pam_clusters, data = datos, ellipse.type = "t",
        repel = TRUE) + theme_bw() + theme(legend.position = "none")\end{verbatim}
    \end{tcolorbox}

    \begin{figure}[h]
        \centering
        \includegraphics[width = 0.35\textwidth]{pam.png}
        \caption{Pam k = 4}
    \end{figure}
    \subsubsection{Clara}
    \begin{tcolorbox}
        \begin{verbatim}
clara_clusters = clara(x = datos, k = 3, metric = "manhattan", stand = TRUE,
    samples = 50, pamLike = TRUE)
clara_clusters
#Grafica
fviz_cluster(object = clara_clusters, ellipse.type = "t", geom = "point") +
        theme_bw() + theme(legend.position = "none")\end{verbatim}
    \end{tcolorbox}

    \begin{figure}[h]
        \centering
        \includegraphics[width = 0.5\textwidth]{clara.png}
        \caption{Clara k = 3}
    \end{figure}
    \subsubsection{K-means Jerarquico}
    \begin{tcolorbox}
        \begin{verbatim}
#Consideramos k = 2
hkmeans_cluster = hkmeans(x = datos, hc.metric = "euclidean",
    hc.method = "complete", k = 2)
fviz_cluster(object = hkmeans_cluster, pallete = "jco", repel = TRUE) +
        theme_bw() + labs(title = "Hierarchical k-means Cluster plot")
#Consideramos k = 3 
hkmeans_cluster = hkmeans(x = datos, hc.metric = "euclidean",
    hc.method = "complete", k = 3)
fviz_cluster(object = hkmeans_cluster, pallete = "jco", repel = TRUE) +
        theme_bw() + labs(title = "Hierarchical k-means Cluster plot")\end{verbatim}
    \end{tcolorbox}

    \begin{figure}[h]
        \begin{minipage}[c]{0.5\textwidth}
            \centering
                \includegraphics[width=0.6\textwidth]{hier1.png}
                \caption{2 Clusters}
        \end{minipage}
        \begin{minipage}[c]{0.5\textwidth}
            \centering
                \includegraphics[width=0.6\textwidth]{hier2.png}
                \caption{4 Clusters}
        \end{minipage}
    \end{figure}
    \subsubsection{Fuzzy Clustering}
    \begin{tcolorbox}[title = input]
        \begin{verbatim}
fuzzy_cluster = fanny(x = datos, diss = FALSE, k = 3,
    metric = "euclidean", stand = FALSE)
#coef de Dunn
fuzzy_cluster$coeff            
        \end{verbatim}
    \end{tcolorbox}
    \begin{tcolorbox}[title = output]
        \begin{verbatim}
> fuzzy_cluster$coeff # alto nivel de fuzzy
dunn_coeff normalized 
 0.4565086  0.2753448\end{verbatim}
    \end{tcolorbox}
    \begin{figure}[h]
        \centering
        \includegraphics[width = 0.6\textwidth]{fuzzy.png}
        \caption{Fuzzy Cluster k = 4}
    \end{figure}
    \subsubsection{Validaci\'on de Clusters}
    Solo aplicaremos criterios de validaci\'on a \textit{kmeans} con k = 2, ya que seg\'un 
    la funci\'on \verb|clValid| es la mejor opci\'on.
    \begin{tcolorbox}[title = input]
        \begin{verbatim}
km_clusters = eclust(x = datos, FUNcluster = "kmeans", k = 2,
    seed = 123, hc_metric = "euclidean", nstart = 50, graph = FALSE)
fviz_silhouette(sil.obj = km_clusters, print.summary = TRUE,
    palette = "jco", ggtheme = theme_classic())
km_clusters$silinfo$clus.avg.widths\end{verbatim}
    \end{tcolorbox}
    \begin{tcolorbox}[title = output]
        \begin{verbatim}
> km_clusters$silinfo$clus.avg.widths
[1] 0.6807337 0.5322582\end{verbatim}
    \end{tcolorbox}
    \newpage
    \begin{figure}[h]
        \centering
        \includegraphics[width = 0.6\textwidth]{validacion.png}
        \caption{Validaci\'on Kmeans k = 2}
    \end{figure}
    Notamos que no hay observaciones que tomen valores negativos, y pocos de estos son cercanos a cero, por lo que muy probablemente las observaciones est\'en bien clasificadas.
\end{document}
